number of samples: 100000
==> training...
normalization constant Z_l is set to 221834.7
normalization constant Z_ab is set to 221362.2
Train: [1][5/391]	BT 0.238 (4.061)	DT 0.000 (2.512)	loss 22.991 (22.950)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][10/391]	BT 2.259 (2.482)	DT 2.026 (1.588)	loss 23.004 (22.964)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][15/391]	BT 0.235 (1.735)	DT 0.000 (1.059)	loss 23.020 (22.964)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][20/391]	BT 0.241 (1.888)	DT 0.000 (1.315)	loss 22.948 (22.958)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][25/391]	BT 8.028 (1.871)	DT 7.775 (1.363)	loss 22.794 (22.955)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][30/391]	BT 0.244 (1.657)	DT 0.000 (1.190)	loss 23.086 (22.948)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][35/391]	BT 0.250 (1.768)	DT 0.000 (1.331)	loss 22.991 (22.951)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][40/391]	BT 0.393 (1.581)	DT 0.154 (1.168)	loss 22.703 (22.925)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][45/391]	BT 0.918 (1.694)	DT 0.680 (1.298)	loss 22.688 (22.911)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][50/391]	BT 0.893 (1.712)	DT 0.582 (1.329)	loss 22.681 (22.897)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][55/391]	BT 0.239 (1.584)	DT 0.000 (1.213)	loss 22.632 (22.870)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][60/391]	BT 0.235 (1.661)	DT 0.000 (1.300)	loss 22.662 (22.860)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][65/391]	BT 9.551 (1.695)	DT 9.306 (1.343)	loss 22.609 (22.846)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][70/391]	BT 0.269 (1.614)	DT 0.000 (1.270)	loss 22.870 (22.832)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][75/391]	BT 0.245 (1.626)	DT 0.000 (1.288)	loss 22.403 (22.814)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][80/391]	BT 0.422 (1.544)	DT 0.132 (1.209)	loss 22.867 (22.802)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][85/391]	BT 0.242 (1.588)	DT 0.000 (1.258)	loss 22.431 (22.783)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][90/391]	BT 1.432 (1.611)	DT 1.103 (1.283)	loss 22.732 (22.774)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][95/391]	BT 0.251 (1.540)	DT 0.000 (1.216)	loss 22.419 (22.763)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][100/391]	BT 0.237 (1.579)	DT 0.000 (1.258)	loss 22.322 (22.744)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][105/391]	BT 6.671 (1.577)	DT 6.409 (1.259)	loss 22.479 (22.732)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][110/391]	BT 0.250 (1.553)	DT 0.000 (1.238)	loss 22.328 (22.716)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][115/391]	BT 0.953 (1.602)	DT 0.719 (1.290)	loss 22.417 (22.704)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][120/391]	BT 0.242 (1.546)	DT 0.000 (1.237)	loss 22.388 (22.692)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][125/391]	BT 0.237 (1.564)	DT 0.000 (1.258)	loss 22.368 (22.680)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][130/391]	BT 0.576 (1.571)	DT 0.271 (1.266)	loss 22.519 (22.669)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][135/391]	BT 0.236 (1.523)	DT 0.000 (1.220)	loss 22.357 (22.657)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][140/391]	BT 0.241 (1.541)	DT 0.000 (1.241)	loss 22.307 (22.642)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][145/391]	BT 6.979 (1.544)	DT 6.729 (1.245)	loss 22.306 (22.630)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][150/391]	BT 0.239 (1.506)	DT 0.000 (1.209)	loss 22.331 (22.618)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][155/391]	BT 0.311 (1.523)	DT 0.001 (1.227)	loss 22.281 (22.605)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][160/391]	BT 0.292 (1.483)	DT 0.000 (1.188)	loss 22.028 (22.594)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][165/391]	BT 0.231 (1.487)	DT 0.000 (1.194)	loss 22.264 (22.582)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][170/391]	BT 2.253 (1.498)	DT 1.986 (1.206)	loss 22.054 (22.569)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][175/391]	BT 0.242 (1.462)	DT 0.000 (1.171)	loss 22.161 (22.557)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][180/391]	BT 0.344 (1.471)	DT 0.001 (1.181)	loss 22.136 (22.545)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][185/391]	BT 5.669 (1.468)	DT 5.434 (1.178)	loss 21.978 (22.532)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][190/391]	BT 0.239 (1.444)	DT 0.000 (1.155)	loss 21.989 (22.518)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][195/391]	BT 0.248 (1.454)	DT 0.000 (1.166)	loss 21.977 (22.505)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][200/391]	BT 0.246 (1.424)	DT 0.000 (1.137)	loss 21.992 (22.494)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][205/391]	BT 0.249 (1.430)	DT 0.000 (1.143)	loss 21.957 (22.480)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][210/391]	BT 0.970 (1.436)	DT 0.720 (1.150)	loss 21.896 (22.468)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][215/391]	BT 0.237 (1.409)	DT 0.000 (1.124)	loss 21.800 (22.456)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][220/391]	BT 0.256 (1.415)	DT 0.000 (1.132)	loss 21.750 (22.443)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][225/391]	BT 6.188 (1.416)	DT 5.955 (1.133)	loss 21.976 (22.433)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][230/391]	BT 0.244 (1.392)	DT 0.000 (1.110)	loss 21.742 (22.421)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][235/391]	BT 0.431 (1.402)	DT 0.191 (1.120)	loss 21.795 (22.409)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][240/391]	BT 0.234 (1.378)	DT 0.000 (1.097)	loss 21.672 (22.396)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][245/391]	BT 0.230 (1.383)	DT 0.000 (1.103)	loss 21.702 (22.382)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][250/391]	BT 0.244 (1.387)	DT 0.000 (1.107)	loss 21.854 (22.370)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][255/391]	BT 0.232 (1.367)	DT 0.000 (1.088)	loss 21.801 (22.359)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][260/391]	BT 0.235 (1.374)	DT 0.000 (1.096)	loss 21.778 (22.347)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][265/391]	BT 6.775 (1.378)	DT 6.536 (1.100)	loss 21.694 (22.334)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][270/391]	BT 0.227 (1.361)	DT 0.000 (1.084)	loss 21.741 (22.323)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][275/391]	BT 1.082 (1.365)	DT 0.836 (1.089)	loss 21.736 (22.310)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][280/391]	BT 0.236 (1.345)	DT 0.000 (1.070)	loss 21.725 (22.299)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][285/391]	BT 0.243 (1.352)	DT 0.000 (1.077)	loss 21.438 (22.286)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][290/391]	BT 0.244 (1.353)	DT 0.000 (1.079)	loss 21.515 (22.274)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][295/391]	BT 0.236 (1.339)	DT 0.000 (1.065)	loss 21.480 (22.261)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][300/391]	BT 0.251 (1.344)	DT 0.000 (1.071)	loss 21.513 (22.249)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][305/391]	BT 5.945 (1.345)	DT 5.681 (1.072)	loss 21.642 (22.235)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][310/391]	BT 0.266 (1.332)	DT 0.000 (1.059)	loss 21.369 (22.223)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][315/391]	BT 1.467 (1.336)	DT 1.217 (1.063)	loss 21.567 (22.211)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][320/391]	BT 0.243 (1.321)	DT 0.000 (1.048)	loss 21.362 (22.199)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][325/391]	BT 0.276 (1.327)	DT 0.000 (1.054)	loss 21.420 (22.188)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][330/391]	BT 0.244 (1.326)	DT 0.000 (1.054)	loss 21.383 (22.177)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][335/391]	BT 0.265 (1.316)	DT 0.000 (1.043)	loss 21.293 (22.164)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][340/391]	BT 0.787 (1.321)	DT 0.554 (1.049)	loss 21.402 (22.151)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][345/391]	BT 6.005 (1.322)	DT 5.768 (1.051)	loss 21.445 (22.139)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][350/391]	BT 0.588 (1.311)	DT 0.349 (1.040)	loss 21.296 (22.127)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][355/391]	BT 1.067 (1.317)	DT 0.831 (1.046)	loss 21.299 (22.116)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][360/391]	BT 0.240 (1.303)	DT 0.000 (1.033)	loss 21.221 (22.102)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][365/391]	BT 0.251 (1.306)	DT 0.000 (1.036)	loss 21.154 (22.091)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][370/391]	BT 0.241 (1.311)	DT 0.000 (1.041)	loss 21.148 (22.078)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][375/391]	BT 0.242 (1.298)	DT 0.007 (1.028)	loss 21.290 (22.066)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][380/391]	BT 0.579 (1.303)	DT 0.339 (1.034)	loss 21.043 (22.053)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][385/391]	BT 5.980 (1.305)	DT 5.736 (1.035)	loss 20.978 (22.041)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [1][390/391]	BT 0.253 (1.296)	DT 0.000 (1.027)	loss 21.211 (22.029)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
epoch 1, total time 506.12
==> training...
Train: [2][5/391]	BT 0.323 (2.684)	DT 0.000 (2.279)	loss 29.185 (30.806)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [2][10/391]	BT 1.965 (2.135)	DT 1.553 (1.752)	loss 23.440 (28.000)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [2][15/391]	BT 0.425 (1.551)	DT 0.000 (1.168)	loss 23.312 (26.789)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [2][20/391]	BT 0.228 (1.552)	DT 0.000 (1.193)	loss 24.096 (26.057)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [2][25/391]	BT 5.620 (1.526)	DT 5.289 (1.167)	loss 23.265 (25.592)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [2][30/391]	BT 0.311 (1.392)	DT 0.001 (1.035)	loss 23.208 (25.188)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [2][35/391]	BT 0.267 (1.471)	DT 0.000 (1.120)	loss 23.032 (24.901)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [2][40/391]	BT 0.247 (1.321)	DT 0.000 (0.980)	loss 22.924 (24.654)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [2][45/391]	BT 0.356 (1.442)	DT 0.000 (1.104)	loss 22.795 (24.448)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [2][50/391]	BT 3.676 (1.539)	DT 3.401 (1.210)	loss 22.571 (24.266)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [2][55/391]	BT 0.267 (1.424)	DT 0.000 (1.100)	loss 22.549 (24.114)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [2][60/391]	BT 0.228 (1.448)	DT 0.000 (1.131)	loss 22.482 (23.980)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [2][65/391]	BT 5.875 (1.442)	DT 5.618 (1.130)	loss 22.351 (23.859)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
Train: [2][70/391]	BT 0.255 (1.438)	DT 0.000 (1.126)	loss 22.349 (23.747)	l_p 0.000 (0.000)	ab_p 0.000 (0.000)
